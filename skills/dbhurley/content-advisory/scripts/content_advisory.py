#!/usr/bin/env python3
# /// script
# requires-python = ">=3.10"
# ///
"""å†…å®¹è¯„çº§ CLI - Kids-In-Mind é£æ ¼çš„ç”µå½±/ç”µè§†å†…å®¹è¯„çº§å·¥å…·ã€‚

æä¾›è¯¦ç»†çš„å†…å®¹åˆ†ç±»ï¼šæ€§/è£¸éœ²ã€æš´åŠ›/è¡€è…¥ã€è¯­è¨€
é‡‡ç”¨ 0-10 åˆ†åˆ¶ï¼Œè¿˜åŒ…æ‹¬ç‰©è´¨ä½¿ç”¨ã€è®¨è®ºè¯é¢˜å’Œæ ¸å¿ƒä¿¡æ¯ã€‚
"""

from __future__ import annotations

import argparse
import html
import json
import os
import re
import sys
from dataclasses import asdict, dataclass, field
from datetime import datetime
from pathlib import Path
from urllib.error import HTTPError, URLError
from urllib.parse import quote_plus, urljoin
from urllib.request import Request, urlopen

# æ•°æ®ç›®å½•
DATA_DIR = Path(os.environ.get("CONTENT_ADVISORY_DATA_DIR", Path.home() / ".clawdbot" / "content-advisory"))
CACHE_FILE = DATA_DIR / "cache.json"

# Kids-In-Mind åŸºç¡€ URL
KIM_BASE = "https://kids-in-mind.com"

USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"


@dataclass
class ContentRating:
    """å†…å®¹è¯„çº§æ•°æ®ç±»"""
    title: str  # ç”µå½±æˆ–ç”µè§†èŠ‚ç›®æ ‡é¢˜
    year: str = ""  # å‘è¡Œå¹´ä»½
    mpaa: str = ""  # MPAA è¯„çº§
    sex_nudity: int = 0  # æ€§/è£¸éœ²è¯„çº§ (0-10)
    violence_gore: int = 0  # æš´åŠ›/è¡€è…¥è¯„çº§ (0-10)
    language: int = 0  # è¯­è¨€è¯„çº§ (0-10)
    sex_nudity_detail: str = ""  # æ€§/è£¸éœ²è¯¦ç»†æè¿°
    violence_gore_detail: str = ""  # æš´åŠ›/è¡€è…¥è¯¦ç»†æè¿°
    language_detail: str = ""  # è¯­è¨€è¯¦ç»†æè¿°
    substance_use: str = ""  # ç‰©è´¨ä½¿ç”¨æè¿°
    discussion_topics: str = ""  # è®¨è®ºè¯é¢˜
    message: str = ""  # æ ¸å¿ƒä¿¡æ¯
    url: str = ""  # è¯„çº§æ¥æº URL
    cached_at: str = ""  # ç¼“å­˜æ—¶é—´
    
    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, d: dict) -> "ContentRating":
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        return cls(**{k: v for k, v in d.items() if k in cls.__dataclass_fields__})


@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç±»"""
    title: str  # æ ‡é¢˜
    year: str  # å¹´ä»½
    url: str  # è¯¦æƒ… URL
    ratings: str = ""  # è¯„çº§å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ "3.5.4"
    mpaa: str = ""  # MPAA è¯„çº§
    
    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)


def load_cache() -> dict[str, ContentRating]:
    """ä» JSON æ–‡ä»¶åŠ è½½ç¼“å­˜"""
    if not CACHE_FILE.exists():
        return {}
    try:
        with open(CACHE_FILE) as f:
            data = json.load(f)
            return {k: ContentRating.from_dict(v) for k, v in data.items()}
    except (json.JSONDecodeError, KeyError):
        return {}


def save_cache(cache: dict[str, ContentRating]) -> None:
    """å°†ç¼“å­˜ä¿å­˜åˆ° JSON æ–‡ä»¶"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    with open(CACHE_FILE, "w") as f:
        json.dump({k: v.to_dict() for k, v in cache.items()}, f, indent=2)


def fetch_url(url: str) -> str:
    """è·å– URL å†…å®¹ä½œä¸ºå­—ç¬¦ä¸²"""
    req = Request(
        url,
        headers={
            "User-Agent": USER_AGENT,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
        },
    )
    try:
        with urlopen(req, timeout=30) as resp:
            return resp.read().decode("utf-8", errors="replace")
    except HTTPError as e:
        raise RuntimeError(f"HTTP {e.code}: {e.reason}") from e
    except URLError as e:
        raise RuntimeError(f"URL é”™è¯¯: {e.reason}") from e


def clean_html(text: str) -> str:
    """ç§»é™¤ HTML æ ‡ç­¾å¹¶è§£ç å®ä½“"""
    # ç§»é™¤è„šæœ¬/æ ·å¼å†…å®¹
    text = re.sub(r"<(script|style)[^>]*>.*?</\1>", "", text, flags=re.DOTALL | re.IGNORECASE)
    # ç§»é™¤æ ‡ç­¾
    text = re.sub(r"<[^>]+>", " ", text)
    # è§£ç å®ä½“
    text = html.unescape(text)
    # è§„èŒƒåŒ–ç©ºç™½
    text = re.sub(r"\s+", " ", text).strip()
    return text


def extract_section_by_id(html_content: str, section_id: str) -> str:
    """ä»å…·æœ‰ç‰¹å®š ID çš„éƒ¨åˆ†æå–æ–‡æœ¬"""
    # æŸ¥æ‰¾å…·æœ‰ id çš„éƒ¨åˆ†ï¼Œç„¶åè·å–å†…å®¹ç›´åˆ°ä¸‹ä¸€ä¸ª h2 æˆ–éƒ¨åˆ†ç»“æŸ
    pattern = rf'id="{section_id}"[^>]*>([^<]*)</h2>\s*</div>\s*</div>\s*<div[^>]*>\s*<div[^>]*>(.*?)</div>'
    match = re.search(pattern, html_content, re.DOTALL | re.IGNORECASE)
    if match:
        content = match.group(2)
        return clean_html(content)[:600]
    
    # å¤‡é€‰æ–¹æ¡ˆï¼šæ›´ç®€å•çš„æ¨¡å¼
    pattern2 = rf'id="{section_id}"[^>]*>.*?</h2>.*?<p[^>]*>(.*?)</p>'
    match2 = re.search(pattern2, html_content, re.DOTALL | re.IGNORECASE)
    if match2:
        return clean_html(match2.group(1))[:600]
    
    return ""


def parse_kim_page(html_content: str, url: str) -> ContentRating:
    """è§£æ Kids-In-Mind è¯„è®ºé¡µé¢"""
    rating = ContentRating(title="", url=url, cached_at=datetime.now().isoformat())
    
    # ä»æ ‡é¢˜æå–ï¼š"Title [Year] [MPAA] - X.Y.Z | Parents' Guide..."
    title_match = re.search(r"<title>([^<]+)</title>", html_content, re.IGNORECASE)
    if title_match:
        title_text = html.unescape(title_match.group(1))
        
        # è§£æï¼š"Greenland 2: Migration [2026] [PG-13] - 1.6.4 | Parents' Guide..."
        main_match = re.match(r"(.+?)\s*\[(\d{4})\]\s*\[([^\]]+)\]\s*-\s*(\d+)\.(\d+)\.(\d+)", title_text)
        if main_match:
            rating.title = main_match.group(1).strip()
            rating.year = main_match.group(2)
            rating.mpaa = main_match.group(3)
            rating.sex_nudity = int(main_match.group(4))
            rating.violence_gore = int(main_match.group(5))
            rating.language = int(main_match.group(6))
        else:
            # å°è¯•æ›´ç®€å•çš„æ¨¡å¼ï¼šåªè·å– | æˆ– [ ä¹‹å‰çš„æ ‡é¢˜
            simple = re.match(r"(.+?)(?:\s*[\|\[]|$)", title_text)
            if simple:
                rating.title = simple.group(1).strip()
    
    # ä½¿ç”¨ ID æå–éƒ¨åˆ†è¯¦æƒ…
    rating.sex_nudity_detail = extract_section_by_id(html_content, "sex")
    rating.violence_gore_detail = extract_section_by_id(html_content, "violence")
    rating.language_detail = extract_section_by_id(html_content, "language")
    
    # æå–ç‰©è´¨ä½¿ç”¨éƒ¨åˆ†
    substance_match = re.search(r'id="substance"[^>]*>.*?SUBSTANCE[^<]*</h2>.*?<p[^>]*>(.*?)</p>', html_content, re.DOTALL | re.IGNORECASE)
    if substance_match:
        rating.substance_use = clean_html(substance_match.group(1))[:400]
    
    # æå–è®¨è®ºè¯é¢˜
    topics_match = re.search(r'id="discussion"[^>]*>.*?DISCUSSION[^<]*</h2>.*?<p[^>]*>(.*?)</p>', html_content, re.DOTALL | re.IGNORECASE)
    if topics_match:
        rating.discussion_topics = clean_html(topics_match.group(1))[:400]
    
    # æå–æ ¸å¿ƒä¿¡æ¯
    message_match = re.search(r'id="message"[^>]*>.*?MESSAGE[^<]*</h2>.*?<p[^>]*>(.*?)</p>', html_content, re.DOTALL | re.IGNORECASE)
    if message_match:
        rating.message = clean_html(message_match.group(1))[:400]
    
    return rating


def search_kim_from_homepage(query: str, limit: int = 10) -> list[SearchResult]:
    """é€šè¿‡æŠ“å– Kids-In-Mind ä¸»é¡µå’Œå­—æ¯ç´¢å¼•é¡µé¢æœç´¢ç”µå½±"""
    results = []
    query_lower = query.lower()
    
    # é¦–å…ˆå°è¯•ç¬¬ä¸€ä¸ªå­—æ¯çš„å­—æ¯ç´¢å¼•é¡µé¢
    first_letter = query_lower[0] if query_lower else "a"
    index_url = f"{KIM_BASE}/{first_letter}.htm"
    
    urls_to_check = [KIM_BASE, index_url]
    seen_urls = set()
    
    for base_url in urls_to_check:
        try:
            html_content = fetch_url(base_url)
            
            # æŸ¥æ‰¾æ‰€æœ‰ç”µå½±é“¾æ¥
            link_pattern = r'href="(/[a-z]/[^"]+\.htm)"[^>]*>([^<]+)'
            for match in re.finditer(link_pattern, html_content, re.IGNORECASE):
                url_path = match.group(1)
                link_text = clean_html(match.group(2))
                
                # è·³è¿‡éç”µå½±é¡µé¢
                if any(skip in url_path.lower() for skip in ["/about", "/contact", "/donate", "/terms", "/search"]):
                    continue
                
                full_url = urljoin(KIM_BASE, url_path)
                if full_url in seen_urls:
                    continue
                seen_urls.add(full_url)
                
                # æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦åŒ¹é…
                if query_lower in link_text.lower():
                    # å°è¯•ä»é“¾æ¥æ–‡æœ¬æˆ– URL ä¸­æå–å¹´ä»½å’Œè¯„çº§
                    year = ""
                    mpaa = ""
                    ratings = ""
                    
                    year_match = re.search(r"\[(\d{4})\]", link_text)
                    if year_match:
                        year = year_match.group(1)
                    
                    mpaa_match = re.search(r"\[(G|PG|PG-13|R|NC-17|NR)\]", link_text)
                    if mpaa_match:
                        mpaa = mpaa_match.group(1)
                    
                    ratings_match = re.search(r"(\d+)\.(\d+)\.(\d+)", link_text)
                    if ratings_match:
                        ratings = f"{ratings_match.group(1)}.{ratings_match.group(2)}.{ratings_match.group(3)}"
                    
                    # æ¸…ç†æ ‡é¢˜
                    title = re.sub(r"\s*\[\d{4}\].*$", "", link_text).strip()
                    
                    results.append(SearchResult(
                        title=title,
                        year=year,
                        url=full_url,
                        ratings=ratings,
                        mpaa=mpaa,
                    ))
                    
                    if len(results) >= limit:
                        return results
        except Exception as e:
            print(f"è·å– {base_url} æ—¶å‡ºé”™: {e}", file=sys.stderr)
            continue
    
    return results


def lookup_title(query: str, year: str | None = None) -> ContentRating | None:
    """æŸ¥æ‰¾æ ‡é¢˜çš„å†…å®¹è¯„çº§"""
    cache = load_cache()
    
    # é¦–å…ˆæ£€æŸ¥ç¼“å­˜
    cache_key = f"{query.lower()}:{year or ''}"
    if cache_key in cache:
        cached = cache[cache_key]
        try:
            cached_time = datetime.fromisoformat(cached.cached_at)
            if (datetime.now() - cached_time).days < 30:
                return cached
        except (ValueError, TypeError):
            pass
    
    # æœç´¢æ ‡é¢˜
    search_results = search_kim_from_homepage(query)
    
    if not search_results:
        return None
    
    # æ‰¾åˆ°æœ€ä½³åŒ¹é…
    query_lower = query.lower()
    best_match = search_results[0]
    
    for result in search_results:
        # ä¼˜å…ˆé€‰æ‹©ç²¾ç¡®æ ‡é¢˜åŒ¹é…
        if result.title.lower() == query_lower:
            best_match = result
            break
        # ä¼˜å…ˆé€‰æ‹©åŒ¹é…å¹´ä»½
        if year and result.year == year:
            best_match = result
            break
    
    # è·å–é¡µé¢
    try:
        html_content = fetch_url(best_match.url)
        rating = parse_kim_page(html_content, best_match.url)
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œå›é€€åˆ°æœç´¢ç»“æœä¿¡æ¯
        if not rating.title:
            rating.title = best_match.title
        if not rating.year and best_match.year:
            rating.year = best_match.year
        if not rating.mpaa and best_match.mpaa:
            rating.mpaa = best_match.mpaa
        if rating.sex_nudity == 0 and best_match.ratings:
            parts = best_match.ratings.split(".")
            if len(parts) == 3:
                rating.sex_nudity = int(parts[0])
                rating.violence_gore = int(parts[1])
                rating.language = int(parts[2])
        
        # ä¿å­˜åˆ°ç¼“å­˜
        cache[cache_key] = rating
        save_cache(cache)
        
        return rating
    except Exception as e:
        print(f"æŸ¥æ‰¾é”™è¯¯: {e}", file=sys.stderr)
        return None


def render_bar(value: int, max_val: int = 10) -> str:
    """ä¸ºè¯„çº§æ¸²æŸ“å¯è§†åŒ–æ¡å½¢å›¾"""
    filled = "â–“" * value
    empty = "â–‘" * (max_val - value)
    return f"{filled}{empty}"


def print_rating(rating: ContentRating, json_output: bool = False) -> None:
    """ä»¥æ ¼å¼åŒ–è¾“å‡ºæ‰“å°å†…å®¹è¯„çº§"""
    if json_output:
        print(json.dumps(rating.to_dict(), indent=2))
        return
    
    # æ ‡é¢˜
    year_str = f" ({rating.year})" if rating.year else ""
    mpaa_str = f" | {rating.mpaa}" if rating.mpaa else ""
    print(f"\nğŸ¬ {rating.title}{year_str}{mpaa_str}\n")
    
    # è¯„çº§æ¡
    print("ğŸ“Š å†…å®¹è¯„çº§")
    print(f"   æ€§/è£¸éœ²:    {rating.sex_nudity:2d} {render_bar(rating.sex_nudity)}")
    print(f"   æš´åŠ›/è¡€è…¥: {rating.violence_gore:2d} {render_bar(rating.violence_gore)}")
    print(f"   è¯­è¨€:      {rating.language:2d} {render_bar(rating.language)}")
    
    # è¯¦æƒ…
    if rating.sex_nudity_detail or rating.violence_gore_detail or rating.language_detail:
        print("\nğŸ“‹ ç±»åˆ«è¯¦æƒ…")
        if rating.sex_nudity_detail:
            detail = rating.sex_nudity_detail[:300]
            print(f"   æ€§/è£¸éœ²: {detail}{'...' if len(rating.sex_nudity_detail) > 300 else ''}")
        if rating.violence_gore_detail:
            detail = rating.violence_gore_detail[:300]
            print(f"   æš´åŠ›: {detail}{'...' if len(rating.violence_gore_detail) > 300 else ''}")
        if rating.language_detail:
            detail = rating.language_detail[:300]
            print(f"   è¯­è¨€: {detail}{'...' if len(rating.language_detail) > 300 else ''}")
    
    if rating.substance_use:
        print(f"\nğŸ’Š ç‰©è´¨ä½¿ç”¨\n   {rating.substance_use[:250]}")
    
    if rating.discussion_topics:
        print(f"\nğŸ’¬ è®¨è®ºè¯é¢˜\n   {rating.discussion_topics[:250]}")
    
    if rating.message:
        print(f"\nğŸ“ æ ¸å¿ƒä¿¡æ¯\n   {rating.message[:250]}")
    
    if rating.url:
        print(f"\nğŸ”— æ¥æº: {rating.url}")
    
    print()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å‘½ä»¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cmd_lookup(args: argparse.Namespace) -> int:
    """æŸ¥æ‰¾ç”µå½±çš„å†…å®¹è¯„çº§"""
    rating = lookup_title(args.title, args.year)
    
    if not rating:
        print(f"âŒ æ— æ³•æ‰¾åˆ° '{args.title}' çš„å†…å®¹è¯„çº§", file=sys.stderr)
        print("   å°è¯•ä¸åŒçš„æ‹¼å†™æˆ–ç›´æ¥æŸ¥çœ‹ kids-in-mind.com", file=sys.stderr)
        return 1
    
    print_rating(rating, args.json)
    return 0


def cmd_search(args: argparse.Namespace) -> int:
    """æœç´¢æ ‡é¢˜"""
    results = search_kim_from_homepage(args.query, args.limit)
    
    if not results:
        print(f"âŒ æœªæ‰¾åˆ° '{args.query}' çš„ç»“æœ", file=sys.stderr)
        return 1
    
    if args.json:
        print(json.dumps([r.to_dict() for r in results], indent=2))
        return 0
    
    print(f"ğŸ” '{args.query}' çš„æœç´¢ç»“æœ:\n")
    for r in results:
        year_str = f" ({r.year})" if r.year else ""
        mpaa_str = f" [{r.mpaa}]" if r.mpaa else ""
        ratings_str = f" - {r.ratings}" if r.ratings else ""
        print(f"  â€¢ {r.title}{year_str}{mpaa_str}{ratings_str}")
    return 0


def cmd_clear_cache(args: argparse.Namespace) -> int:
    """æ¸…é™¤ç¼“å­˜"""
    if CACHE_FILE.exists():
        CACHE_FILE.unlink()
        print("ğŸ—‘ï¸  ç¼“å­˜å·²æ¸…é™¤")
    else:
        print("â„¹ï¸  ç¼“å­˜å·²ç»ä¸ºç©º")
    return 0


def main() -> int:
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="å†…å®¹è¯„çº§ - Kids-In-Mind é£æ ¼çš„ç”µå½±è¯„çº§",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    subparsers = parser.add_subparsers(dest="command", help="å‘½ä»¤")
    
    # lookup å‘½ä»¤
    p_lookup = subparsers.add_parser("lookup", help="æŸ¥æ‰¾ç”µå½±çš„å†…å®¹è¯„çº§")
    p_lookup.add_argument("title", help="ç”µå½±æˆ–èŠ‚ç›®æ ‡é¢˜")
    p_lookup.add_argument("--year", "-y", help="å‘è¡Œå¹´ä»½")
    p_lookup.add_argument("--json", action="store_true", help="JSON è¾“å‡º")
    p_lookup.set_defaults(func=cmd_lookup)
    
    # search å‘½ä»¤
    p_search = subparsers.add_parser("search", help="æœç´¢æ ‡é¢˜")
    p_search.add_argument("query", help="æœç´¢æŸ¥è¯¢")
    p_search.add_argument("--limit", "-n", type=int, default=10, help="æœ€å¤§ç»“æœæ•°")
    p_search.add_argument("--json", action="store_true", help="JSON è¾“å‡º")
    p_search.set_defaults(func=cmd_search)
    
    # clear-cache å‘½ä»¤
    p_clear = subparsers.add_parser("clear-cache", help="æ¸…é™¤ç¼“å­˜ç»“æœ")
    p_clear.set_defaults(func=cmd_clear_cache)
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 0
    
    return args.func(args)


if __name__ == "__main__":
    raise SystemExit(main())
